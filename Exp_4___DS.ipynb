{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rieesteves/EXP-4--DS/blob/main/Exp_4___DS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "w91zEm48D6dS",
        "outputId": "55a6da91-ebf7-405b-a562-616f2592c412"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 5s 0us/step\n",
            "Epoch 1/25\n",
            "782/782 [==============================] - 444s 562ms/step - loss: 1.5205 - accuracy: 0.4601 - val_loss: 1.2044 - val_accuracy: 0.5618\n",
            "Epoch 2/25\n",
            "782/782 [==============================] - 405s 518ms/step - loss: 1.0836 - accuracy: 0.6192 - val_loss: 1.0068 - val_accuracy: 0.6504\n",
            "Epoch 3/25\n",
            "782/782 [==============================] - 396s 506ms/step - loss: 0.9419 - accuracy: 0.6715 - val_loss: 0.8850 - val_accuracy: 0.6893\n",
            "Epoch 4/25\n",
            "782/782 [==============================] - 400s 512ms/step - loss: 0.8466 - accuracy: 0.7066 - val_loss: 0.7841 - val_accuracy: 0.7286\n",
            "Epoch 5/25\n",
            "782/782 [==============================] - 393s 503ms/step - loss: 0.7807 - accuracy: 0.7288 - val_loss: 0.8118 - val_accuracy: 0.7241\n",
            "Epoch 6/25\n",
            "782/782 [==============================] - 393s 503ms/step - loss: 0.7236 - accuracy: 0.7466 - val_loss: 0.8581 - val_accuracy: 0.7069\n",
            "Epoch 7/25\n",
            "782/782 [==============================] - 393s 502ms/step - loss: 0.6637 - accuracy: 0.7659 - val_loss: 0.7199 - val_accuracy: 0.7513\n",
            "Epoch 8/25\n",
            "782/782 [==============================] - 393s 502ms/step - loss: 0.6273 - accuracy: 0.7795 - val_loss: 0.7715 - val_accuracy: 0.7387\n",
            "Epoch 9/25\n",
            "782/782 [==============================] - 393s 503ms/step - loss: 0.5860 - accuracy: 0.7929 - val_loss: 0.7259 - val_accuracy: 0.7528\n",
            "Epoch 10/25\n",
            "782/782 [==============================] - 411s 525ms/step - loss: 0.5483 - accuracy: 0.8058 - val_loss: 0.6971 - val_accuracy: 0.7679\n",
            "Epoch 11/25\n",
            "782/782 [==============================] - 418s 535ms/step - loss: 0.5284 - accuracy: 0.8137 - val_loss: 0.7134 - val_accuracy: 0.7631\n",
            "Epoch 12/25\n",
            "782/782 [==============================] - 434s 555ms/step - loss: 0.5030 - accuracy: 0.8223 - val_loss: 0.7159 - val_accuracy: 0.7626\n",
            "Epoch 13/25\n",
            "782/782 [==============================] - 436s 558ms/step - loss: 0.4836 - accuracy: 0.8297 - val_loss: 0.6885 - val_accuracy: 0.7720\n",
            "Epoch 14/25\n",
            "137/782 [====>.........................] - ETA: 5:45 - loss: 0.4520 - accuracy: 0.8427"
          ]
        }
      ],
      "source": [
        "import numpy\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "# from tensorflow.keras.utils import np_utils\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "# Set random seed for purposes of reproducibility\n",
        "seed = 21\n",
        "from keras.datasets import cifar10\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "# Loading in the data\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "# Normalise the inputs from 0-255 to between 0 and 1 by dividing by 255\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "# One-hot encode outputs\n",
        "y_train = to_categorical(y_train, num_classes=10)  # Assuming 10 classes in CIFAR-10\n",
        "y_test = to_categorical(y_test, num_classes=10)    # Assuming 10 classes in CIFAR-10\n",
        "class_num = y_test.shape[1]\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Conv2D(32, (3, 3), input_shape=X_train.shape[1:], padding='same'))\n",
        "model.add(keras.layers.Activation('relu'))\n",
        "model.add(keras.layers.Conv2D(64, 3, activation='relu', padding='same'))\n",
        "model.add(keras.layers.MaxPooling2D(2))\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Conv2D(128, 3, activation='relu', padding='same'))\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "model.add(keras.layers.Dense(32, activation='relu'))\n",
        "model.add(keras.layers.Dropout(0.3))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Dense(class_num, activation='softmax'))\n",
        "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', 'val_accuracy'])\n",
        "# print(model.summary())\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "numpy.random.seed(seed)\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=25, batch_size=64)\n",
        "# Model evaluation\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "pd.DataFrame(history.history).plot()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bvzfIji6D_Pb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f79ed1d-b086-410e-abd5-b586ddac0a7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "300/300 - 4s - loss: 0.2835 - accuracy: 0.9196 - val_loss: 0.1419 - val_accuracy: 0.9574 - 4s/epoch - 12ms/step\n",
            "Epoch 2/10\n",
            "300/300 - 3s - loss: 0.1096 - accuracy: 0.9688 - val_loss: 0.0939 - val_accuracy: 0.9718 - 3s/epoch - 10ms/step\n",
            "Epoch 3/10\n",
            "300/300 - 4s - loss: 0.0703 - accuracy: 0.9797 - val_loss: 0.0741 - val_accuracy: 0.9770 - 4s/epoch - 14ms/step\n",
            "Epoch 4/10\n",
            "300/300 - 5s - loss: 0.0494 - accuracy: 0.9855 - val_loss: 0.0678 - val_accuracy: 0.9789 - 5s/epoch - 16ms/step\n",
            "Epoch 5/10\n",
            "300/300 - 4s - loss: 0.0368 - accuracy: 0.9892 - val_loss: 0.0633 - val_accuracy: 0.9807 - 4s/epoch - 15ms/step\n",
            "Epoch 6/10\n",
            "300/300 - 4s - loss: 0.0261 - accuracy: 0.9930 - val_loss: 0.0688 - val_accuracy: 0.9793 - 4s/epoch - 13ms/step\n",
            "Epoch 7/10\n",
            "300/300 - 3s - loss: 0.0197 - accuracy: 0.9951 - val_loss: 0.0626 - val_accuracy: 0.9793 - 3s/epoch - 11ms/step\n",
            "Epoch 8/10\n",
            "300/300 - 3s - loss: 0.0146 - accuracy: 0.9968 - val_loss: 0.0653 - val_accuracy: 0.9806 - 3s/epoch - 10ms/step\n",
            "Epoch 9/10\n",
            "300/300 - 3s - loss: 0.0116 - accuracy: 0.9973 - val_loss: 0.0625 - val_accuracy: 0.9806 - 3s/epoch - 9ms/step\n",
            "Epoch 10/10\n",
            "300/300 - 3s - loss: 0.0081 - accuracy: 0.9986 - val_loss: 0.0622 - val_accuracy: 0.9808 - 3s/epoch - 10ms/step\n",
            "Baseline Error: 1.92%\n",
            "Epoch 1/10\n",
            "300/300 [==============================] - 20s 65ms/step - loss: 0.2525 - accuracy: 0.9269 - val_loss: 0.0813 - val_accuracy: 0.9756\n",
            "Epoch 2/10\n",
            "300/300 [==============================] - 21s 69ms/step - loss: 0.0802 - accuracy: 0.9761 - val_loss: 0.0602 - val_accuracy: 0.9799\n",
            "Epoch 3/10\n",
            "300/300 [==============================] - 19s 63ms/step - loss: 0.0565 - accuracy: 0.9832 - val_loss: 0.0420 - val_accuracy: 0.9863\n",
            "Epoch 4/10\n",
            "300/300 [==============================] - 20s 67ms/step - loss: 0.0431 - accuracy: 0.9872 - val_loss: 0.0393 - val_accuracy: 0.9872\n",
            "Epoch 5/10\n",
            "300/300 [==============================] - 19s 64ms/step - loss: 0.0346 - accuracy: 0.9895 - val_loss: 0.0346 - val_accuracy: 0.9881\n",
            "Epoch 6/10\n",
            "300/300 [==============================] - 20s 67ms/step - loss: 0.0292 - accuracy: 0.9907 - val_loss: 0.0375 - val_accuracy: 0.9861\n",
            "Epoch 7/10\n",
            "300/300 [==============================] - 19s 65ms/step - loss: 0.0258 - accuracy: 0.9917 - val_loss: 0.0331 - val_accuracy: 0.9885\n",
            "Epoch 8/10\n",
            "300/300 [==============================] - 20s 65ms/step - loss: 0.0198 - accuracy: 0.9936 - val_loss: 0.0320 - val_accuracy: 0.9894\n",
            "Epoch 9/10\n",
            "300/300 [==============================] - 20s 66ms/step - loss: 0.0178 - accuracy: 0.9943 - val_loss: 0.0312 - val_accuracy: 0.9904\n",
            "Epoch 10/10\n",
            "300/300 [==============================] - 19s 63ms/step - loss: 0.0161 - accuracy: 0.9945 - val_loss: 0.0323 - val_accuracy: 0.9889\n",
            "CNN Error: 1.11%\n",
            "Epoch 1/10\n",
            "300/300 [==============================] - 20s 66ms/step - loss: 0.4184 - accuracy: 0.8686 - val_loss: 0.0904 - val_accuracy: 0.9732\n",
            "Epoch 2/10\n",
            "300/300 [==============================] - 21s 69ms/step - loss: 0.1005 - accuracy: 0.9702 - val_loss: 0.0516 - val_accuracy: 0.9831\n",
            "Epoch 3/10\n",
            "300/300 [==============================] - 21s 70ms/step - loss: 0.0710 - accuracy: 0.9782 - val_loss: 0.0431 - val_accuracy: 0.9861\n",
            "Epoch 4/10\n",
            "300/300 [==============================] - 20s 65ms/step - loss: 0.0566 - accuracy: 0.9828 - val_loss: 0.0350 - val_accuracy: 0.9892\n",
            "Epoch 5/10\n",
            "300/300 [==============================] - 21s 69ms/step - loss: 0.0481 - accuracy: 0.9847 - val_loss: 0.0304 - val_accuracy: 0.9908\n",
            "Epoch 6/10\n",
            "300/300 [==============================] - 20s 65ms/step - loss: 0.0419 - accuracy: 0.9868 - val_loss: 0.0287 - val_accuracy: 0.9908\n",
            "Epoch 7/10\n",
            "300/300 [==============================] - 21s 69ms/step - loss: 0.0392 - accuracy: 0.9877 - val_loss: 0.0288 - val_accuracy: 0.9909\n",
            "Epoch 8/10\n",
            "300/300 [==============================] - 21s 69ms/step - loss: 0.0328 - accuracy: 0.9897 - val_loss: 0.0289 - val_accuracy: 0.9903\n",
            "Epoch 9/10\n",
            "300/300 [==============================] - 20s 65ms/step - loss: 0.0312 - accuracy: 0.9901 - val_loss: 0.0271 - val_accuracy: 0.9907\n",
            "Epoch 10/10\n",
            "300/300 [==============================] - 21s 69ms/step - loss: 0.0293 - accuracy: 0.9905 - val_loss: 0.0271 - val_accuracy: 0.9910\n",
            "Large CNN Error: 0.90%\n"
          ]
        }
      ],
      "source": [
        "### Baseline Model with Multi-Layer Perceptrons\n",
        "# Baseline MLP for MNIST dataset\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "# flatten 28*28 images to a 784 vector for each image\n",
        "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
        "X_train = X_train.reshape((X_train.shape[0], num_pixels)).astype('float32')\n",
        "X_test = X_test.reshape((X_test.shape[0], num_pixels)).astype('float32')\n",
        "# normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "# one hot encode outputs\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "# define baseline model\n",
        "def baseline_model():\n",
        " # create model\n",
        " model = Sequential()\n",
        " model.add(Dense(num_pixels, input_shape=(num_pixels,), kernel_initializer='normal', activation='relu'))\n",
        " model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
        " # Compile model\n",
        " model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        " return model\n",
        "# build the model\n",
        "model = baseline_model()\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Simple Convolutional Neural Network for MNIST\n",
        "# Simple CNN for the MNIST Dataset\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "# reshape to be [samples][width][height][channels]\n",
        "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32')\n",
        "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32')\n",
        "# normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "# one hot encode outputs\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "# define a simple CNN model\n",
        "def baseline_model():\n",
        " # create model\n",
        " model = Sequential()\n",
        " model.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
        " model.add(MaxPooling2D())\n",
        " model.add(Dropout(0.2))\n",
        " model.add(Flatten())\n",
        " model.add(Dense(128, activation='relu'))\n",
        " model.add(Dense(num_classes, activation='softmax'))\n",
        " # Compile model\n",
        " model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        " return model\n",
        "# build the model\n",
        "model = baseline_model()\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Larger Convolutional Neural Network for MNIST\n",
        "# Larger CNN for the MNIST Dataset\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "# reshape to be [samples][width][height][channels]\n",
        "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32')\n",
        "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32')\n",
        "# normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "# one hot encode outputs\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "# define the larger model\n",
        "def larger_model():\n",
        " # create model\n",
        " model = Sequential()\n",
        " model.add(Conv2D(30, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
        " model.add(MaxPooling2D())\n",
        " model.add(Conv2D(15, (3, 3), activation='relu'))\n",
        " model.add(MaxPooling2D())\n",
        " model.add(Dropout(0.2))\n",
        " model.add(Flatten())\n",
        " model.add(Dense(128, activation='relu'))\n",
        " model.add(Dense(50, activation='relu'))\n",
        " model.add(Dense(num_classes, activation='softmax'))\n",
        " # Compile model\n",
        " model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        " return model\n",
        "# build the model\n",
        "model = larger_model()\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UJZV9f4AJ_ZX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwKScK1nf4IOtPEJRL2ozx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}